

第六章、Spark的高级算子

一、mapPartitionsWithIndex
	1、作用：把RDD中每个分区的分区号和对应的值取出来，进行操作
	2、定义：  def mapPartitionsWithIndex[U: ClassTag](f: (Int, Iterator[T]) => Iterator[U],
	    参数：Int----> 分区号，从0开始
		      Iterator[T] ----> 返回某个分区中的所有元素
	3、Demo：
	   创建一个RDD：val rdd1 = sc.parallelize(List(1,2,3,4,5,6,7,8,9),2)
	   
	   创建一个自定义函数：返回分区号中的所有元素
			def func1(index:Int,iter:Iterator[Int]): Iterator[String]={
				iter.toList.map(x=>"[PartID:"+index +",value="+x+ "]").iterator
			}

			结果：[PartID:0,value=1]
			
	  使用mapPartitionsWithIndex，
	  rdd1.mapPartitionsWithIndex(func1).collect
	  返回结果：
	  [PartID:0,value=1], [PartID:0,value=2], [PartID:0,value=3], [PartID:0,value=4], 
	  [PartID:1,value=5], [PartID:1,value=6], [PartID:1,value=7], [PartID:1,value=8], [PartID:1,value=9]
	  
二、aggregate
	1、先对局部进行聚合，再对全局进行聚合
			def aggregate[U: ClassTag](zeroValue: U)(seqOp: (U, T) => U, combOp: (U, U) => U): U 
	                                     初始值       局部操作           全局操作
	
	2、Demo
	    val rdd1 = sc.parallelize(List(1,2,3,4,5),2)
		查看每个分区中的数据
		rdd1.mapPartitionsWithIndex(func1).collect
		
		[PartID:0,value=1], [PartID:0,value=2], 
		[PartID:1,value=3], [PartID:1,value=4], [PartID:1,value=5]
		
		(1) 需求：求每个分区中的最大值，再求和
		          2 + 5 = 7
			初始值：0
			rdd1.aggregate(0)(math.max(_,_),_+_)    ---> 7
			
			初始值：10
			rdd1.aggregate(10)(math.max(_,_),_+_)   ---> 30 
			
		(2) 一个字符串的例子
		     val rdd2 = sc.parallelize(List("a","b","c","d","e","f"),2)
			改造func1，接受字符串
				def func2(index:Int,iter:Iterator[String]): Iterator[String]={
					iter.toList.map(x=>"[PartID:"+index +",value="+x+ "]").iterator
				}
			确定每个分区中的字符串：
				rdd2.mapPartitionsWithIndex(func2).collect
				[PartID:0,value=a], [PartID:0,value=b], [PartID:0,value=c], 
				[PartID:1,value=d], [PartID:1,value=e], [PartID:1,value=f]
				
			对字符串进行聚合操作：
			   rdd2.aggregate("")(_+_,_+_)    ----> abcdef
			   rdd2.aggregate("|")(_+_,_+_)   ----> ||abc|def
			 
三、aggregateByKey: 执行聚合操作，数据类型是<key,value>
	1、准备数据
	    val pairRDD = sc.parallelize(List( ("cat",2), ("cat", 5), ("mouse", 4),("cat", 12), ("dog", 12), ("mouse", 2)), 2)

	2、改造一下func2函数，接受<key,value>
		def func3(index:Int,iter:Iterator[(String,Int)]): Iterator[String]={
			iter.toList.map(x=>"[PartID:"+index +",value="+x+ "]").iterator
		}
		
	3、查看pairRDD中每个分区的数据
	     pairRDD.mapPartitionsWithIndex(func3).collect
		 [PartID:0,value=(cat,2)], [PartID:0,value=(cat,5)], [PartID:0,value=(mouse,4)], 
		 [PartID:1,value=(cat,12)], [PartID:1,value=(dog,12)], [PartID:1,value=(mouse,2)]

	4、Demo	
		（1）将每个分区中动物最多的个数求和
		     pairRDD.aggregateByKey(0)(math.max(_,_),_+_).collect
			 结果：(dog,12), (cat,17), (mouse,6)
			 
		（2）将每种动物的个数求和
	
		      pairRDD.aggregateByKey(0)(_+_,_+_).collect
				结果：(dog,12), (cat,19), (mouse,6)
				
			也可以使用reduceByKey操作：
			  pairRDD.reduceByKey(_+_).collect
		
	
四、coalesce与repartition
	1、都是将RDD中的分区进行重分区
	2、区别：
	    coalesce： 默认不会进行Shuffle(false)
		repartition：进行Shuffle操作
		即：是否会将数据真正通过网络进行重分区
		
	3、Demo演示：
	     val pairRDD = sc.parallelize(List( ("cat",2), ("cat", 5), ("mouse", 4),("cat", 12), ("dog", 12), ("mouse", 2)), 2)
	
		进行重分区
		 val rdd1 = pairRDD.repartition(3)   -----> 查看rdd1分区的长度：3
		 
		 val rdd2 = pairRDD.coalesce(3)       ----> 查看rdd2分区的长度：2 
		 
		 val rdd3 = pairRDD.coalesce(3,true)  ----> 查看rdd3分区的长度：3
	

五、其他高级算子
	参考：http://homepage.cs.latrobe.edu.au/zhe/ZhenHeSparkRDDAPIExamples.html 
	
	
	
	
	
	
	
	
	
	
